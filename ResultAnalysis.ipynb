{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support, multilabel_confusion_matrix, plot_confusion_matrix, classification_report\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import trange\n",
    "import time\n",
    "import pprint\n",
    "import datetime\n",
    "import argparse\n",
    "from scipy.stats import gmean\n",
    "import yaml\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "import utils\n",
    "from featureExtractor import load_audio_file, get_mel_spectrogram, modify_file_variable_length\n",
    "from dataLoader import get_label_files, DataGeneratorPatch, PatchGeneratorPerFile\n",
    "from model import CNN_LeakyReLU, CNN_LSTM_LeakyReLU, CNN_LSTM_Att_LeakyReLU, CNN_LSTM_Att_ReLU\n",
    "import test\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read parameters file from yaml passed by argument\n",
    "params = yaml.load(open(\"params.yaml\"))\n",
    "params_dataset = params['dataset']\n",
    "params_extract = params['extract']\n",
    "params_learn = params['learn']\n",
    "params_pred = params['predictive']\n",
    "\n",
    "suffix_in = params['suffix'].get('in')\n",
    "suffix_out = params['suffix'].get('out')\n",
    "\n",
    "params_extract['audio_len_samples'] = int(params_extract.get('fs') * params_extract.get('audio_len_s'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================== PATHS FOR DATA, FEATURES and GROUND TRUTH\n",
    "# where to look for the dataset\n",
    "path_root_data = params_dataset.get('dataset_path')\n",
    "\n",
    "params_path = {'path_to_features': os.path.join(path_root_data, 'features'),\n",
    "               'featuredir_tr': 'audio_train_varup2/',\n",
    "               'featuredir_te': 'audio_test_varup2/',\n",
    "               'path_to_dataset': path_root_data,\n",
    "               'audiodir_tr': 'train/',\n",
    "               'audiodir_te': 'test/',\n",
    "               'audio_shapedir_tr': 'audio_train_shapes/',\n",
    "               'audio_shapedir_te': 'audio_test_shapes/',\n",
    "               'gt_files': os.path.join(path_root_data, 'Metadata')}\n",
    "\n",
    "\n",
    "params_path['featurepath_tr'] = os.path.join(params_path.get('path_to_features'), params_path.get('featuredir_tr'))\n",
    "params_path['featurepath_te'] = os.path.join(params_path.get('path_to_features'), params_path.get('featuredir_te'))\n",
    "\n",
    "params_path['audiopath_tr'] = os.path.join(params_path.get('path_to_dataset'), params_path.get('audiodir_tr'))\n",
    "params_path['audiopath_te'] = os.path.join(params_path.get('path_to_dataset'), params_path.get('audiodir_te'))\n",
    "\n",
    "params_path['audio_shapepath_tr'] = os.path.join(params_path.get('path_to_dataset'),\n",
    "                                                 params_path.get('audio_shapedir_tr'))\n",
    "params_path['audio_shapepath_te'] = os.path.join(params_path.get('path_to_dataset'),\n",
    "                                                 params_path.get('audio_shapedir_te'))\n",
    "\n",
    "params_files = {'gt_test': os.path.join(params_path.get('gt_files'), 'Drill_Dataset_Test.csv'),\n",
    "                'gt_train': os.path.join(params_path.get('gt_files'), 'Drill_Dataset_Train.csv')}\n",
    "\n",
    "# # ============================================= print all params to keep record in output file\n",
    "print('params_files=')\n",
    "pprint.pprint(params_files, width=1, indent=4)\n",
    "print('params_extract=')\n",
    "pprint.pprint(params_extract, width=1, indent=4)\n",
    "print('params_learn=')\n",
    "pprint.pprint(params_learn, width=1, indent=4)\n",
    "print('params_pred=')\n",
    "pprint.pprint(params_pred, width=1, indent=4)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(params_files.get('gt_train'))\n",
    "test_csv = pd.read_csv(params_files.get('gt_test'))\n",
    "filelist_audio_tr = train_csv.fname.values.tolist()\n",
    "filelist_audio_te = test_csv.fname.values.tolist()\n",
    "\n",
    "file_to_label = {params_path.get('audiopath_tr') + k: v for k, v in\n",
    "                 zip(train_csv.fname.values, train_csv.label.values)}\n",
    "\n",
    "list_labels = sorted(list(set(train_csv.label.values)))\n",
    "\n",
    "label_to_int = {k: v for v, k in enumerate(list_labels)}\n",
    "int_to_label = {v: k for k, v in label_to_int.items()}\n",
    "\n",
    "file_to_int = {k: label_to_int[v] for k, v in file_to_label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_list_tr = [f for f in os.listdir(params_path.get('featurepath_tr')) if f.endswith(suffix_in + '.data') and\n",
    "                  os.path.isfile(os.path.join(params_path.get('featurepath_tr'), f.replace(suffix_in, suffix_out)))]\n",
    "\n",
    "labels_audio_train = get_label_files(filelist=ff_list_tr,\n",
    "                                     dire=params_path.get('featurepath_tr'),\n",
    "                                     suffix_in=suffix_in,\n",
    "                                     suffix_out=suffix_out\n",
    "                                     )\n",
    "\n",
    "print('Number of clips considered as train set: {0}'.format(len(ff_list_tr)))\n",
    "print('Number of labels loaded for train set: {0}'.format(len(labels_audio_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_files, val_files = train_test_split(ff_list_tr,\n",
    "                                       test_size=params_learn.get('val_split'),\n",
    "                                       stratify=labels_audio_train,\n",
    "                                       random_state=42\n",
    "                                       )\n",
    "\n",
    "tr_gen_patch = DataGeneratorPatch(feature_dir=params_path.get('featurepath_tr'),\n",
    "                                  file_list=tr_files,\n",
    "                                  params_learn=params_learn,\n",
    "                                  params_extract=params_extract,\n",
    "                                  suffix_in='_mel',\n",
    "                                  suffix_out='_label',\n",
    "                                  floatx=np.float32\n",
    "                                  )\n",
    "\n",
    "val_gen_patch = DataGeneratorPatch(feature_dir=params_path.get('featurepath_tr'),\n",
    "                                   file_list=val_files,\n",
    "                                   params_learn=params_learn,\n",
    "                                   params_extract=params_extract,\n",
    "                                   suffix_in='_mel',\n",
    "                                   suffix_out='_label',\n",
    "                                   floatx=np.float32,\n",
    "                                   scaler=tr_gen_patch.scaler\n",
    "                                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import CNN_LeakyReLU, CNN_LSTM_LeakyReLU, CNN_LSTM_Att_LeakyReLU, CNN_LSTM_Att_ReLU\n",
    "model2 = CNN_LSTM_Att_LeakyReLU(params_learn=params_learn, params_extract=params_extract)\n",
    "model2.load_weights('weights/dumy.hdf5')\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nCompute predictions on test set:==================================================\\n')\n",
    "\n",
    "list_preds = []\n",
    "\n",
    "te_files = [f for f in os.listdir(params_path.get('featurepath_te')) if f.endswith(suffix_in + '.data')]\n",
    "\n",
    "te_preds = np.empty((len(te_files), params_learn.get('n_classes')))\n",
    "\n",
    "te_gen_patch = PatchGeneratorPerFile(feature_dir=params_path.get('featurepath_te'),\n",
    "                                     file_list=te_files,\n",
    "                                     params_extract=params_extract,\n",
    "                                     suffix_in='_mel',\n",
    "                                     floatx=np.float32,\n",
    "                                     scaler=tr_gen_patch.scaler\n",
    "                                     )\n",
    "\n",
    "for i in trange(len(te_files), miniters=int(len(te_files) / 100), ascii=True, desc=\"Predicting...\"):\n",
    "    patches_file = te_gen_patch.get_patches_file()\n",
    "\n",
    "    preds_patch_list = model2.predict(patches_file).tolist()\n",
    "    preds_patch = np.array(preds_patch_list)\n",
    "\n",
    "    if params_recog.get('aggregate') == 'gmean':\n",
    "        preds_file = gmean(preds_patch, axis=0)\n",
    "    else:\n",
    "        print('unkown aggregation method for prediction')\n",
    "    te_preds[i, :] = preds_file\n",
    "\n",
    "\n",
    "list_labels = np.array(list_labels)\n",
    "pred_label_files_int = np.argmax(te_preds, axis=1)\n",
    "pred_labels = [int_to_label[x] for x in pred_label_files_int]\n",
    "\n",
    "te_files_wav = [f.replace(suffix_in + '.data', '.wav') for f in os.listdir(params_path.get('featurepath_te'))\n",
    "                if f.endswith(suffix_in + '.data')]\n",
    "pred = pd.DataFrame(te_files_wav, columns=[\"fname\"])\n",
    "pred['label'] = pred_labels\n",
    "\n",
    "print('\\nEvaluate ACC and print score============================================================================')\n",
    "\n",
    "# read ground truth\n",
    "gt_test = pd.read_csv(params_files.get('gt_test'))\n",
    "\n",
    "# init Evaluator object\n",
    "evaluator = test.Evaluator(gt_test, pred, list_labels, params_ctrl, params_files)\n",
    "\n",
    "print('\\n=============================ACCURACY===============================================================')\n",
    "print('=============================ACCURACY===============================================================\\n')\n",
    "evaluator.evaluate_acc()\n",
    "evaluator.evaluate_acc_classwise()\n",
    "evaluator.print_summary_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_test1 = gt_test.sort_values([\"fname\"])\n",
    "gt_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = pred.sort_values([\"fname\"])\n",
    "pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(gt_test1['label'], pred1['label'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "cmn = (cm.astype('float') / cm.sum(axis=1)[:, np.newaxis])*100\n",
    "\n",
    "ax = plt.subplots(figsize=(8, 5.5))[1]\n",
    "sn.heatmap(cmn.T, cmap='flare', annot=True, square=True, linecolor='black', linewidths=0.75, ax = ax, fmt = '.2f', annot_kws={'size': 16})\n",
    "ax.set_xlabel('Predicted', fontsize=18, fontweight='bold')\n",
    "ax.xaxis.set_label_position('bottom')\n",
    "ax.xaxis.set_ticklabels([\"Broken\", \"Normal\", \"Other\"], fontsize=16)\n",
    "ax.set_ylabel('Ground Truth', fontsize=18, fontweight='bold')\n",
    "ax.yaxis.set_ticklabels([\"Broken\", \"Normal\", \"Other\"], fontsize=16)\n",
    "# plt.title('Confusion matrix', fontsize=20, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"results/AugmentedDataset18Aug_Split_183_early_att_ori.png\", bbox_inches='tight', dpi=300)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr2 = classification_report(gt_test1['label'], pred1['label'])\n",
    "print(cr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
